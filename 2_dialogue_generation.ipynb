{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNJvrN3LzQ6Fk/2+hgJTn1F",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tgeral68/Binary-Stochastic-Representations-for-Large-Multi-class-Classification/blob/master/2_dialogue_generation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iVXycoM-vzC0",
        "outputId": "bde35f8e-ad8f-439d-eaaa-1d15b25b3f5c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.16.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.13.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.23.5)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (10.0.1)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.15)\n",
            "Requirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.19.4 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.20.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.4->datasets) (4.9.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2024.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "! pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "from matplotlib import pyplot as plt\n",
        "from collections import Counter"
      ],
      "metadata": {
        "id": "4gMctwE0v12A"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "dataset = load_dataset(\"multi_woz_v22\", trust_remote_code=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dSnXat8sv3a8",
        "outputId": "20827573-2211-4590-b96b-72eb10471648"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"openai-community/gpt2\")\n",
        "tokenizer.add_special_tokens({'pad_token': '<|endoftext|>'})\n",
        "model = AutoModelForCausalLM.from_pretrained(\"openai-community/gpt2\")\n",
        "model.resize_token_embeddings(len(tokenizer))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2XKKKfs6wUE5",
        "outputId": "37bbfd14-a4c8-4983-918c-f9ab0db743c8"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Embedding(50257, 768)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implement the dataset module\n",
        "\n",
        "Create an object having as parent `torch.utils.data.dataset` implementing that return previous turn and answer of the dataset."
      ],
      "metadata": {
        "id": "MFyuuKf4xcWK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "\n",
        "class WoZWindowedGenerationDataset:\n",
        "    def __init__(self, dataset, window_size=3):\n",
        "        self.dataset = dataset\n",
        "        self.window_size = window_size\n",
        "        self.index = []\n",
        "        for i, dial in enumerate(dataset):\n",
        "            for j, speaker in enumerate(dial['turns']['speaker']):\n",
        "                if speaker == 1:\n",
        "                    self.index.append((i,j))\n",
        "    def __len__(self):\n",
        "        return len(self.index)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        i, j = self.index[index]\n",
        "        dial = self.dataset[i]['turns']['utterance']\n",
        "\n",
        "        turns = dial[j-1] if(j!= 0) else ''\n",
        "        answer = dial[j]\n",
        "        return {'turns': turns,\n",
        "                'answer': answer}\n",
        "\n",
        "class WoZHistoryWindowedGenerationDataset:\n",
        "  def __init__(self, dataset, window_size=3):\n",
        "      self.dataset = dataset\n",
        "      self.window_size = window_size\n",
        "      self.index = []\n",
        "      for i, dial in enumerate(dataset):\n",
        "          for j, speaker in enumerate(dial['turns']['speaker']):\n",
        "              if speaker == 1:\n",
        "                  self.index.append((i,j))\n",
        "  def __len__(self):\n",
        "      return len(self.index)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "      i, j = self.index[index]\n",
        "      dial = self.dataset[i]['turns']['utterance']\n",
        "      spea = self.dataset[i]['turns']['speaker']\n",
        "\n",
        "      utterance = dial[max(j-self.window_size, 0):j]\n",
        "      speaker = spea[max(j-self.window_size, 0):j]\n",
        "\n",
        "      answer = dial[j]\n",
        "      return {'utterance': utterance,\n",
        "              'speaker': speaker,\n",
        "              'answer': answer}\n"
      ],
      "metadata": {
        "id": "euhCckxZxNGN"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DialogueCollator(Dataset):\n",
        "    def __init__(self, tokenizer):\n",
        "        self.tokenizer = tokenizer\n",
        "    def __call__(self, data):\n",
        "        input_tokens = self.tokenizer(['[USER]' + d['turns'] + \"[BOT]\" + d['answer'] for d in data],\n",
        "                                 return_tensors='pt', return_length=True, padding=True)\n",
        "        return {\n",
        "            'input_ids': input_tokens.input_ids,\n",
        "            'attention_mask': input_tokens.attention_mask\n",
        "        }\n",
        "class DialogueHistoryCollator(Dataset):\n",
        "    def __init__(self, tokenizer):\n",
        "        self.tokenizer = tokenizer\n",
        "    def __call__(self, data):\n",
        "        text = [''.join([('[USER]' if(speaker == 0) else '[BOT]') + turn for speaker, turn in zip(d['speaker'],d['utterance'])]) + \"[BOT]\" + d['answer'] for d in data]\n",
        "        input_tokens = self.tokenizer(text,\n",
        "                                 return_tensors='pt', return_length=True, padding=True)\n",
        "        return {\n",
        "            'input_ids': input_tokens.input_ids,\n",
        "            'attention_mask': input_tokens.attention_mask\n",
        "        }"
      ],
      "metadata": {
        "id": "KBvXWOVdyoFr"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm.notebook import trange, tqdm\n",
        "from torch import optim\n",
        "from torch import nn\n",
        "\n",
        "\n",
        "class Trainer():\n",
        "    def __init__(self, model, padding_idx=100):\n",
        "        self.model = model\n",
        "        self.optimizer = None\n",
        "\n",
        "    def at_training_start(self, learning_rate = 1e-3):\n",
        "        self.optimizer = optim.Adam(self.model.parameters(), lr=learning_rate)\n",
        "        self.criterion = nn.CrossEntropyLoss(ignore_index=50257)\n",
        "\n",
        "    def validation_step(self, data):\n",
        "        pass\n",
        "\n",
        "    def training_step(self, data):\n",
        "        y_pred = self.model(**data)\n",
        "        y_truth = data[\"input_ids\"][:, 1:].flatten()\n",
        "\n",
        "        loss_reconstruction = self.criterion(y_pred.logits[:,:-1].reshape(y_truth.shape[0], -1), y_truth)\n",
        "        (loss_reconstruction).backward()\n",
        "        return loss_reconstruction.item()\n",
        "\n",
        "    def on_validation_end(self, resp):\n",
        "        pass\n",
        "\n",
        "    def validation(self, validation_dl):\n",
        "        pass\n",
        "\n",
        "    def fit(self,\n",
        "            training_dl,\n",
        "            validation_dl,\n",
        "            learning_rate = 1e-3,\n",
        "            validation_frequency = 8,\n",
        "            max_iter = 10000,\n",
        "            use_gpu=False,\n",
        "\n",
        "        ):\n",
        "        if(use_gpu):\n",
        "          self.model = self.model.cuda()\n",
        "        self.at_training_start(learning_rate)\n",
        "\n",
        "        iter_count = 0\n",
        "        loss_buffer = []\n",
        "        pbar = trange(max_iter)\n",
        "\n",
        "        while(iter_count < max_iter):\n",
        "            for data in training_dl:\n",
        "                if use_gpu:\n",
        "                    data = {k:v.cuda() for k, v in data.items()}\n",
        "                self.optimizer.zero_grad()\n",
        "                loss_buffer += [self.training_step(data)]\n",
        "                self.optimizer.step()\n",
        "\n",
        "                if(iter_count  % validation_frequency == 0):\n",
        "                    print(\"Loss at iteration %s is %s\"%(iter_count, np.mean(loss_buffer)))\n",
        "                    self.validation(validation_dl)\n",
        "                    loss_buffer = []\n",
        "                iter_count += 1\n",
        "                pbar.update(1)\n",
        "                if(iter_count < max_iter):\n",
        "                  break\n",
        "\n",
        "        self.model = self.model.cpu()"
      ],
      "metadata": {
        "id": "ipmEgeiT0rAD"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_set = WoZHistoryWindowedGenerationDataset(dataset['train'])"
      ],
      "metadata": {
        "id": "fvsGtfrnpE9N"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "collator = DialogueHistoryCollator(tokenizer)\n",
        "training_dl = DataLoader(training_set, batch_size=16, shuffle=True, collate_fn=collator, num_workers=2)"
      ],
      "metadata": {
        "id": "ZNNjwKBmnfZk"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "my_trainer = Trainer(model)\n",
        "my_trainer.fit(training_dl, None, validation_frequency=250, use_gpu=True, max_iter=2000)"
      ],
      "metadata": {
        "id": "_wvY1TMAn1dh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Chatbot(object):\n",
        "  def __init__(self):\n",
        "    pass\n",
        "\n",
        "  def answer(self, current_input):\n",
        "    return \"Not Implemented\"\n",
        "\n",
        "  def start(self):\n",
        "    current_answer = \"Start dialogue\"\n",
        "    current_input = \"\"\n",
        "    while(current_input != 'exit'):\n",
        "      current_input = input(\"Bot: \"+current_answer + \" \\nUser: \")\n",
        "      current_answer = self.answer(current_input)\n",
        "\n",
        "class ChitChat(Chatbot):\n",
        "  def __init__(self, model, tokenizer, collator, history_len = 1):\n",
        "    self.model = model\n",
        "    self.tokenizer = tokenizer\n",
        "    self.utterance = []\n",
        "    self.hlen = history_len\n",
        "\n",
        "  def answer(self, current_input):\n",
        "    self.utterance.append('[USER]'+current_input)\n",
        "    # print(''.join(self.utterance[max(0, len(self.utterance) - self.hlen): ]))\n",
        "    tokenized_text = self.tokenizer(''.join(self.utterance[max(0, len(self.utterance) - self.hlen): ]), return_tensors='pt')\n",
        "    generated_token_ids = self.model.generate(**tokenized_text, do_sample=True, max_length=200, pad_token_id=model.config.eos_token_id)[0]\n",
        "    answer = self.tokenizer.decode(generated_token_ids).split('[BOT]')[-1][:-len('<|endoftext|>')]\n",
        "    self.utterance.append('[BOT]'+answer)\n",
        "    return answer\n"
      ],
      "metadata": {
        "id": "6ymfF5TorsLW"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cb = ChitChat(model.cpu(), tokenizer, collator, history_len=3)"
      ],
      "metadata": {
        "id": "Xkv9SHaxxoiJ"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cb.start()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JZV9GhzDxyW0",
        "outputId": "192058cd-20a2-4ac6-b2e8-442ae0a89397"
      },
      "execution_count": 42,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Bot: Start dialogue \n",
            "User: Hello I'am looking for a restaurant in cambridge, what can you propose?\n",
            "Bot: There are many expensive restaurants in Cambridge. Is there a certain area of town you prefer? \n",
            "User: Not particularly, but I would  like one not to expensive, maybe an italian restaurant if there is one\n",
            "Bot: I have about 5, I can try to book a reservation for you? \n",
            "User: What are the name of those restaurant?\n",
            "Bot: Okay I will try to book it for you. Do you require anything specific? \n",
            "User: Can you give me the name of the restaurant ?\n",
            "Bot: The Curry Prince is on 451 Newmarket Road Fen Ditton. Would you like to book a booking? \n",
            "User: What are the price?\n",
            "Bot: The price is 10.10 pounds. Can I help you with anything else? \n",
            "User: Ok let's book it for 2 people at 8pm\n",
            "Bot: Unfortunately. The booking was unsuccessful and you may want to try for a different day or time? \n",
            "User: At 9pm\n",
            "Bot: I have made your reservation for Cote at 10 pm for 2 people at 8: pm. Your reference number is OB9JX1Y. Do you need a taxi to get from the station to the restaurant? \n",
            "User: No thanks\n",
            "Bot: Awesome thing, have a nice day. Good bye. \n",
            "User: exit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MduRYjM0x0Bk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}